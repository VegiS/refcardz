= Kubernetes Refcardz
:toc:
:toc-placement!:

toc::[]

== What is Kubernetes?

Kubernetes (http://kubernetes.io) is an open source orchestration system for managing containerized applications across multiple hosts, providing basic mechanisms for deployment, maintenance, and scaling of applications.

Kubernetes, or "`k8s`" in short, allows the user to provide declarative primitives for the desired state, for example “need 3 Couchbase servers running”. Kubernetes self-healing mechanisms, such as auto-restarting, re-scheduling, and replicating containers then ensure that this state is met.

Kubernetes supports Docker and Rocket containers, and other container image formats and container runtimes will be supported in the future.

== Key Concepts of Kubernetes

=== Cluster

A Kubernetes cluster is a set of physical or virtual machines and other infrastructure resources that are used running your applications. The machines that manage the cluster are called _Master_ and the machines that run the containers are called _Worker_.

=== Node

A Node is a physical or virtual machine. It has the necessary services to run application containers.

A Master Node is the central control point that provides a unified view of the cluster. Multiple masters can be setup to create a high-availability cluster.

A Worker Node run tasks as delegated by the master. Worker node can run one or more pods.

=== Pod

A Pod is the smallest deployable units that can be created, scheduled, and managed. Its a logical collection of containers that belong to an application.

Each resource in Kubernetes is defined using a configuration file. For example, a Couchbase pod can be defined as:

[source, text]
----
apiVersion: v1
kind: Pod
# labels attached to this Pod
metadata:
  name: couchbase-pod
spec:
  containers:
  - name: couchbase
    # Docker image that will run in this Pod
    image: couchbase
    ports:
    - containerPort: 8091
----

=== Kubelet

Kubelet is a service running on each Node that allows to run containers and be managed from the master. This service reads container manifests as YAML or JSON files that describes a Pod. A typical way to provide this manifest is using the configuration file as shown in the previous section. Kubelet ensures that the containers defined in the Pods are started and continue running.

Kubelet is a Kubernetes-internal concept and generally does not require direct manipulation. 

An overview of Kubernetes showing the key components is shown:

image::kubernetes-architecture.png[]

=== Label

A label is a key/value pair that is attached to objects, such as pods. Multiple labels can be attached to a resource. Labels can be used to organize and to select subsets of objects. In the previous example

=== Replication Controller (RC)

A replication controller ensures that a specified number of pod replicas are running on worker nodes at all times. It allows both up- and down-scaling the number of replicas. It also ensures recreation of pod when the worker node reboots or otherwise fails.

A Replication Controller creating two instances of a Couchbase pod can be defined as:

[source, text]
----
apiVersion: v1
kind: ReplicationController
metadata:
  name: couchbase-controller
spec:
  # Two replicas of the Pod to be created
  replicas: 2
  # Identifies the label key and value on the Pod that
  # this Replication Controller is responsible for managing
  selector:
    app: couchbase-rc-pod
  # 'cookie cutter' used for creating new pods when necessary
  template:
    metadata:
      labels:
        # label key and value on the pod. These must match the selector above.
        app: couchbase-rc-pod
    spec:
      containers:
      - name: couchbase
        image: couchbase
        ports:
        - containerPort: 8091
----

=== Service

Each Pod is assigned a unique IP address. If the Pod is inside a Replication Controller then it is recreated but may be given a different IP address. This makes it difficult for an application server, such as WildFly, to access a database, such as Couchbase, using its IP address.

A Service defines a logical set of Pods and a policy by which to access them. IP address assigned to a Service does not change over time, and thus can be relied upon by other Pods. Typically the Pods belonging to a Service are defined by a Label Selector.

For example, a Couchbase service might be defined as:

[source, text]
----
apiVersion: v1
kind: Service
metadata: 
  name: couchbase-service
  labels: 
    app: couchbase-service-pod
spec: 
  ports:
    - port: 8091
  # label keys and values of the Pod started elsewhere
  selector: 
    app: couchbase-rc-pod
----

Note that the labels used in `selector` must match the metadata used for creating the Pod by the Replication Controller.

=== Volumes

A Volume is a directory on disk or in another container. A volume outlives any containers that run within the Pod, and data is preserved across Container restarts. The directory, the medium that backs it, and the contents of it are determined by the particular volume type used.

Multiple types of volumes are supported. Some of the commonly used volume types are shown below:

[options="header"]
|====
| Volume Type | Mounts into your pod
| `hostPath` | A file or directory from the host node's filesystem
| `nfs` | Existing Network File System share
| `awsElasticBlockStore` | An Amazon Web Service EBS Volume
| `gcePersistentDisk` | A Google Compute Engine Persistent Disk
|====

A Volume is specified in the Pod configuration file as shown:

[source, text]
----
apiVersion: v1
kind: ReplicationController
metadata:
  name: couchbase-controller
spec:
  replicas: 1
  # In-line template of the Pod
  template:
    metadata:
      app: couchbase-rc-pod
    spec:
      containers:
        - name: couchbase-rc-pod
          image: arungupta/couchbase
          ports:
          - containerPort: 8091
          volumeMounts:
          # name must match the volume name below
          - name: nfs
            mountPath: /usr/share/couchbase
      volumes:
        - name: nfs
          persistentVolumeClaim:
            claimName: nfs
----

This configuration file also shows that Pod template can be specified inline.

== Getting Started with Kubernetes

=== Kubectl CLI

`kubectl` is a command-line utility that controls the Kubernetes cluster. This utility can be used in the following format:

`kubectl [command] [type] [name] [flags]`

- `[command]` specifies the operation that needs to be performed on the resource. For example, `create`, `describe`, `delete`, or `scale`.
- `[type]` specifies the Kubernetes resource type. For example, `pod`, `service`, `replicationcontroller`, or `node`. Resource types are case-sensitive and you can specify the singular, plural, or abbreviated forms.
- `[name]` Specifies the name of the resource. Names are case-sensitive. If the name is omitted, details for all resources are displayed, for example `kubectl get pods`	.

Some examples of `kubectl` commands and their purpose:

[options="header"]
|====
| Command | Purpose
| `kubectl create -f couchbase-pod.yml` | Create a Couchbase pod
| `kubectl create -f couchbase-rc.yml` | Create a Couchbase Replication Controller
| `kubectl get pods` | List all the pods
| `kubectl describe pod couchbase-pod` | Describe the Couchbase pod
|====

`kubectl --help` shows the complete list of available commands.

=== Start the Kubernetes Cluster

Kubernetes cluster can be started in multiple ways. The most common ones are using Vagrant, Amazon Web Service (AWS), Google Compute Engine (GCE), and Azure. http://kubernetes.io/v1.1/docs/getting-started-guides/README.html provides complete details about different options.

Latest Kubernetes release can be downloaded from https://github.com/kubernetes/kubernetes/releases/latest. This includes the binary to start the cluster and the `kubectl` script to manage this cluster.

Alternatively the cluster can also be started as `curl -sS https://get.k8s.io | bash`.

The `KUBERNETES_PROVIDER` environment variable defines which variant to use. Cluster can be started as:

[source, text]
----
./cluster/kube-up.sh
----

Additional worker nodes can be created by setting the environment variable `NUM_MINIONS`, for example:

[source, text]
----
export NUM_MINIONS=6
----

Cluster can be shutdown as:

[source, text]
----
./cluster/kube-down.sh
----

Variant specific configuration for Vagrant, Amazon, and Google are shown next.

==== Start the Cluster using Vagrant

Running Kubernetes with Vagrant is an easy way run, develop and test on your local machine.

Kubernetes cluster using Vagrant can be started as:

[source, text]
----
export KUBERNETES_PROVIDER=vagrant
./cluster/kube-up.sh
----

By default, the Vagrant will create two Fedora VMs - one for the master node and one for the worker node. Status of the created VMs can be seen using `vagrant status` command, for example:

[source, text]
----
vagrant status
Current machine states:

master                    running (virtualbox)
minion-1                  running (virtualbox)
----

By default, each VM is assigned 1GB memory. A different number can be assigned by setting `KUBERNETES_MEMORY` environment variable, for example:

[source, text]
----
export KUBERNETES_MEMORY=2048
----

Complete instructions ro run and manage a Kubernetes cluster using Vagrant are available at: http://kubernetes.io/v1.1/docs/getting-started-guides/vagrant.html.

==== Start the Cluster using AWS

Running Kubernetes with AWS requires:

- AWS account
- Install and configure AWS CLI
- AWS instance and profile with EC2 full access

Set `KUBERNETES_PROVIDER` to `aws` as:

[source, text]
----
export KUBERNETES_PROVIDER=aws
----

Start and configure the cluster as explained earlier.

By default, the script will provision a new VPC and a 4 node Kubernetes cluster in `us-west-2a` (Oregon) with `t2.micro` instances running on Ubuntu. These, and other values, such as memory for Master and Worker node, can be configured in `cluster/aws/config-default.sh`.

==== Start the Cluster using GCE

Running Kubernetes with GCE requires:

- Google Cloud Platform account with billing enabled
- Install and configure Google Cloud SDK as explained at http://kubernetes.io/v1.1/docs/getting-started-guides/gce.html

Either unset `KUBERNETES_PROVIDER` or set it to `gce` as:

[source, text]
----
export KUBERNETES_PROVIDER=gce
----

Start and configure the cluster as explained earlier.

By default, the script will provision a single Master node and 4 Worker nodes in `us-central1-b` zone with `n1-standard-1` instances running on Debian. These, and other values, such as memory for Master and Worker node, can be configured in `cluster/gce/config-default.sh`.

== Run your first Container

A Container can be started on Kubernetes cluster using `kubectl`. Easiest way is to specify the Docker image name to the `run` command:

[source, text]
----
kubectl.sh run couchbase --image=arungupta/couchbase
----

This command will start a pre-configured Couchbase container in a Pod wrapped inside a Replication Controller. Status of this RC can be seen:

[source, text]
----
kubectl.sh get rc
CONTROLLER   CONTAINER(S)   IMAGE(S)              SELECTOR        REPLICAS   AGE
couchbase    couchbase      arungupta/couchbase   run=couchbase   1          16s
----

Status of the Pod can be seen:

[source, text]
----
kubectl.sh get po
NAME              READY     STATUS    RESTARTS   AGE
couchbase-0s8lx   1/1       Running   0          1m
----

Alternatively, the Container can also be started using the configuration file:

[source, text]
----
kubectl.sh create -f couchbase-pod.yaml
----

The file `couchbase-pod.yaml` contains the Pod definition as explained earlier.

== Scale Applications

Pods in a RC can be scaled up and down:

[source, text]
----
kubectl.sh scale --replicas=3 rc couchbase
replicationcontroller "couchbase" scaled
----

Updated number of replicas can be seen:

[source, text]
----
kubectl.sh get rc
CONTROLLER   CONTAINER(S)   IMAGE(S)              SELECTOR        REPLICAS   AGE
couchbase    couchbase      arungupta/couchbase   run=couchbase   3          3m
----

Note, the updated replicas is 3 here.

== Application using Multiple Containers

Typically applications consists of a "`frontend`" and a "`backend`". 

== Storing Data using Volumes

== Health Check

== About the Author

Arun Gupta is the vice president of developer advocacy at Couchbase. He has been building developer communities for 10+ years at Sun, Oracle, and Red Hat. He has deep expertise in leading cross-functional teams to develop and execute strategy, planning and execution of content, marketing campaigns, and programs. Prior to that he led engineering teams at Sun and is a founding member of the Java EE team.

Gupta has authored more than 2,000 blog posts on technology. He has extensive speaking experience in more than 40 countries on myriad topics and is a JavaOne Rock Star. Gupta also founded the Devoxx4Kids chapter in the US and continues to promote technology education among children. An author of a best-selling book, an avid runner, a globe trotter, a Docker Captain, a Java Champion, and a JUG leader, he is easily accessible at @arungupta.
